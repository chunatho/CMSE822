==1607== Memcheck, a memory error detector
==1607== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.
==1607== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info
==1607== Command: ./test 100
==1607== 
==1604== Memcheck, a memory error detector
==1604== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.
==1604== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info
==1604== Command: ./test 100
==1604== 
==1606== Memcheck, a memory error detector
==1606== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.
==1606== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info
==1606== Command: ./test 100
==1606== 
==1605== Memcheck, a memory error detector
==1605== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al.
==1605== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info
==1605== Command: ./test 100
==1605== 
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[dev-intel16:1607] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
==1607== 
==1607== HEAP SUMMARY:
==1607==     in use at exit: 307,542 bytes in 2,652 blocks
==1607==   total heap usage: 6,103 allocs, 3,451 frees, 843,165 bytes allocated
==1607== 
==1607== LEAK SUMMARY:
==1607==    definitely lost: 34 bytes in 2 blocks
==1607==    indirectly lost: 0 bytes in 0 blocks
==1607==      possibly lost: 0 bytes in 0 blocks
==1607==    still reachable: 307,508 bytes in 2,650 blocks
==1607==         suppressed: 0 bytes in 0 blocks
==1607== Rerun with --leak-check=full to see details of leaked memory
==1607== 
==1607== For counts of detected and suppressed errors, rerun with: -v
==1607== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 8 from 6)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[dev-intel16:1604] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
==1604== 
==1604== HEAP SUMMARY:
==1604==     in use at exit: 307,542 bytes in 2,652 blocks
==1604==   total heap usage: 6,103 allocs, 3,451 frees, 843,165 bytes allocated
==1604== 
==1604== LEAK SUMMARY:
==1604==    definitely lost: 34 bytes in 2 blocks
==1604==    indirectly lost: 0 bytes in 0 blocks
==1604==      possibly lost: 0 bytes in 0 blocks
==1604==    still reachable: 307,508 bytes in 2,650 blocks
==1604==         suppressed: 0 bytes in 0 blocks
==1604== Rerun with --leak-check=full to see details of leaked memory
==1604== 
==1604== For counts of detected and suppressed errors, rerun with: -v
==1604== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 8 from 6)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[dev-intel16:1605] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
==1605== 
==1605== HEAP SUMMARY:
==1605==     in use at exit: 307,542 bytes in 2,652 blocks
==1605==   total heap usage: 6,103 allocs, 3,451 frees, 843,165 bytes allocated
==1605== 
==1605== LEAK SUMMARY:
==1605==    definitely lost: 34 bytes in 2 blocks
==1605==    indirectly lost: 0 bytes in 0 blocks
==1605==      possibly lost: 0 bytes in 0 blocks
==1605==    still reachable: 307,508 bytes in 2,650 blocks
==1605==         suppressed: 0 bytes in 0 blocks
==1605== Rerun with --leak-check=full to see details of leaked memory
==1605== 
==1605== For counts of detected and suppressed errors, rerun with: -v
==1605== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 8 from 6)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[dev-intel16:1606] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
==1606== 
==1606== HEAP SUMMARY:
==1606==     in use at exit: 307,542 bytes in 2,652 blocks
==1606==   total heap usage: 6,103 allocs, 3,451 frees, 843,165 bytes allocated
==1606== 
==1606== LEAK SUMMARY:
==1606==    definitely lost: 34 bytes in 2 blocks
==1606==    indirectly lost: 0 bytes in 0 blocks
==1606==      possibly lost: 0 bytes in 0 blocks
==1606==    still reachable: 307,508 bytes in 2,650 blocks
==1606==         suppressed: 0 bytes in 0 blocks
==1606== Rerun with --leak-check=full to see details of leaked memory
==1606== 
==1606== For counts of detected and suppressed errors, rerun with: -v
==1606== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 8 from 6)
